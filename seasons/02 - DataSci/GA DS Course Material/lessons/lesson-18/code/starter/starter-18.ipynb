{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"../../../../data/stumbleupon.tsv\", sep = \"\\t\")\n",
    "data[\"title\"] = data.boilerplate.map(lambda x: json.loads(x).get(\"title\", \"\"))\n",
    "data[\"body\"] = data.boilerplate.map(lambda x: json.loads(x).get(\"body\", \"\"))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting \"Greenness\" of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set comes from [stumbleupon](https://www.stumbleupon.com/), a web page recommender.\n",
    "\n",
    "A description of the columns is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FieldName|Type|Description\n",
    "---------|----|-----------\n",
    "url|string|Url of the webpage to be classified\n",
    "title|string|Title of the article\n",
    "body|string|Body text of article\n",
    "urlid|integer| StumbleUpon's unique identifier for each url\n",
    "boilerplate|json|Boilerplate text\n",
    "alchemy_category|string|Alchemy category (per the publicly available Alchemy API found at [www.alchemyapi.com](www.alchemyapi.com))\n",
    "alchemy_category_score|double|Alchemy category score (per the publicly available Alchemy API found at [www.alchemyapi.com](www.alchemyapi.com))\n",
    "avglinksize| double|Average number of words in each link\n",
    "commonlinkratio_1|double|# of links sharing at least 1 word with 1 other links / # of links\n",
    "commonlinkratio_2|double|# of links sharing at least 1 word with 2 other links / # of links\n",
    "commonlinkratio_3|double|# of links sharing at least 1 word with 3 other links / # of links\n",
    "commonlinkratio_4|double|# of links sharing at least 1 word with 4 other links / # of links\n",
    "compression_ratio|double|Compression achieved on this page via gzip (measure of redundancy)\n",
    "embed_ratio|double|Count of number of embed usage\n",
    "frameBased|integer (0 or 1)|A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "frameTagRatio|double|Ratio of iframe markups over total number of markups\n",
    "hasDomainLink|integer (0 or 1)|True (1) if it contains an `<a>` with an url with domain\n",
    "html_ratio|double|Ratio of `<html>` tags vs text in the page\n",
    "image_ratio|double|Ratio of `<img>` tags vs text in the page\n",
    "is_news|integer (0 or 1) | True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "lengthyLinkDomain| integer (0 or 1)|True (1) if at least 3 `<a>`'s text contains more than 30 alphanumeric characters\n",
    "linkwordscore|double|Percentage of words on the page that are in hyperlink's text\n",
    "news_front_page| integer (0 or 1)|True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "non_markup_alphanum_characters|integer| Page's text's number of alphanumeric characters\n",
    "numberOfLinks|integer Number of <a>|markups\n",
    "numwords_in_url| double|Number of words in url\n",
    "parametrizedLinkRatio|double|A link is parametrised if it is url contains parameters or has an attached onClick event\n",
    "spelling_errors_ratio|double|Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "label|integer (0 or 1)|User-determined label. Either evergreen (1) or non-evergreen (0); available for train.tsv only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Use of the CountVectorizer\n",
    "We previously used the `CountVectorizer` to extract text features for this classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = data[\"title\"].fillna(\"\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectoriser = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range = (1, 2),\n",
    "                             stop_words = \"english\",\n",
    "                             binary = True)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectoriser.fit(titles)\n",
    "\n",
    "# Use `transform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectoriser.transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Build a model to predict evergreeness of a website\n",
    "Then we used those features to build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression(penalty = \"l1\")\n",
    "y = data[\"label\"]\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring = \"roc_auc\")\n",
    "print(\"CV AUC {}, Average AUC {}\".format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Demonstration: Pipelines\n",
    "Often we will want to combine these steps to evaluate on some future dataset. For that incoming, future dataset, we need to make sure we perform the **exact same** transformations on the data. If `has_brownies_in_text` is column 19, we need to make sure it is column 19 when it comes to evaluation time.\n",
    "\n",
    "Pipelines combine all of the pre-processing steps and model building into a single object.\n",
    "\n",
    "Rather than manually evaluating the transformers and then feeding them into the model, pipelines tie these steps together. Similar to models and vectorizers in scikit-learn, they are equipped with `fit` and `predict` or `predict_proba` methods as any model would be, but they ensure the proper data transformations are performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        (\"features\", vectoriser),\n",
    "        (\"model\", model)   \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set\n",
    "training_data = data[:6000]\n",
    "X_train = training_data[\"title\"].fillna(\"\")\n",
    "y_train = training_data[\"label\"]\n",
    "\n",
    "# These rows are rows obtained in the future, unavailable at training time\n",
    "X_new = data[6000:][\"title\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the full pipeline\n",
    "# This means we perform the steps laid out above\n",
    "# First we fit the vectorizer, \n",
    "# and then feed the output of that into the fit function of the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here again we apply the full pipeline for predictions\n",
    "# The text is transformed automatically to match the features from the pipeline\n",
    "pipeline.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add a `MaxAbsScaler` scaling step to the pipeline as well, this should occur after the vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <Code Here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <Code Here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Additionally, we want to merge many different feature sets automatically, we can use `FeatureUnion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
